{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#標準ライブラリ\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "from xml.etree import ElementTree as etree\n",
    "\n",
    "#サードパーティ(要インストール)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lxml.html\n",
    "import requests as req\n",
    "from concurrent import futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html型式から変換する関数\n",
    "def transform_html(url):\n",
    "    html_data = req.get(url)\n",
    "    converted_data = lxml.html.fromstring(html_data.content)\n",
    "    \n",
    "    return converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ページ数計算\n",
    "ページ数が不明であるため、サイト内の求人広告総数を１ページあたりの広告数で割ることで算出\n",
    "'''\n",
    "def get_last_page(root_url):\n",
    "    url_sum_xpath = './/span[@class=\"c-text -highlightNumber\"]/text()' #求人広告総数\n",
    "    page_url_xpath = './/li[@class=\"p-contentList__jobList__item\"]' #１ページあたりの求人広告数\n",
    "    converted_data = transform_html(root_url)\n",
    "\n",
    "    page_url = converted_data.findall(page_url_xpath)\n",
    "    url_sum = converted_data.xpath(url_sum_xpath)\n",
    "    url_sum = url_sum[0]\n",
    "    url_sum = url_sum.replace(',','')\n",
    "\n",
    "    page_num = int(url_sum)//len(page_url)+1 #あまりがでるので小数点以下切り捨てして１ページ分追加\n",
    "\n",
    "\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "このサイトの求人全てを取得するには、地域ごとから取得することによりできる。\n",
    "'''\n",
    "ROOT_URL = 'https://04510.jp/jobs/areas'\n",
    "tiiki_list = ['tohoku','kanto','tokai','kansai','hokuriku','chugoku','kyushu']\n",
    "url_list = []\n",
    "url_xpath = './/div[@class=\"p-cardJob__button\"]/a[@class=\"c-button -detail\"]'\n",
    "\n",
    "for i in range(0,7):\n",
    "    tiiki = tiiki_list[i]\n",
    "    tiiki_url = '/'.join([ROOT_URL,tiiki])\n",
    "    last_page = get_last_page(tiiki_url)\n",
    "    page_num = 1\n",
    "    while page_num <= last_page:\n",
    "        param = '?page={}'.format(page_num)\n",
    "        combined_url = '/'.join([tiiki_url,param])\n",
    "        converted_data = transform_html(combined_url)\n",
    "        try:\n",
    "            for element in converted_data.findall(url_xpath):\n",
    "                url_list.append(element.get('href'))\n",
    "            print(tiiki+':'+str(page_num))\n",
    "        except:\n",
    "            print('fail:page '+tiiki)\n",
    "        page_num +=1\n",
    "        # time.sleep(1)\n",
    "\n",
    "url_df = pd.DataFrame(url_list,columns=[\"url\"])\n",
    "url_df.to_csv('url.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv('url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "取得したURLから求人広告の概要を取得するための関数\n",
    "必要ないデータもまとめて取得してあとからいらないものを削除するようにしています\n",
    "'''\n",
    "def get_data(i):\n",
    "\n",
    "    hr_dict = {}\n",
    "    url = url_df['url'][i]\n",
    "    converted_data = transform_html(url)\n",
    "    \n",
    "    job_requirement_dt_xpath = './/div[@class=\"p-informationListView\"]/dl/dt/h4' #辞書のキーにする部分\n",
    "    job_requirement_dt = converted_data.findall(job_requirement_dt_xpath)\n",
    "\n",
    "    job_requirement_dd_xpath = './/div[@class=\"p-informationListView\"]/dl/dd' #辞書のバリューにする部分\n",
    "    job_requirement_dd = converted_data.findall(job_requirement_dd_xpath)\n",
    "\n",
    "    for job_requirement_dt, job_requirement_dd in zip(job_requirement_dt, job_requirement_dd):\n",
    "        job_requirement_dd_text = ''\n",
    "        job_requirement_dt_text = job_requirement_dt.text\n",
    "        job_requirement_dd_texts = job_requirement_dd.itertext() #バリューの指定したxpathの子階層のテキストも全取得する\n",
    "        for dd_text in job_requirement_dd_texts:\n",
    "            job_requirement_dd_text += dd_text\n",
    "        hr_dict.setdefault(job_requirement_dt_text, job_requirement_dd_text)\n",
    "\n",
    "    title = converted_data.xpath('.//h2[@class=\"c-indexLevel2\"]/text()')\n",
    "    hr_dict.setdefault('ページタイトル',title)\n",
    "\n",
    "\n",
    "    # get_day = '{}/{}/{}'.format(year, month, day)\n",
    "    baitai = '???'\n",
    "\n",
    "    hr_dict.setdefault('ページurl', url)\n",
    "    # hr_dict.setdefault('取得日', get_day)\n",
    "    hr_dict.setdefault('媒体名', baitai)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    return hr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "並行処理で取得していく\n",
    "joblibも使用したことがあるがこちらのほうが止まることなくスムーズに処理ができた\n",
    "'''\n",
    "myThreads = []\n",
    "with futures.ThreadPoolExecutor(max_workers=24) as executor:\n",
    "    for i in range(0,30000):\n",
    "        try: \n",
    "            myThreads.append(executor.submit(get_data,i))\n",
    "        except:\n",
    "            print('error')\n",
    "            continue\n",
    "\n",
    "result = []\n",
    "for x in myThreads:\n",
    "    try:\n",
    "        item = x.result()\n",
    "        result.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
